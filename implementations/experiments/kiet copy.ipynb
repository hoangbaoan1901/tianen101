{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 00m 21s]\n",
      "val_loss: 0.003270822111517191\n",
      "\n",
      "Best val_loss So Far: 0.003270822111517191\n",
      "Total elapsed time: 00h 00m 53s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "224               |96                |units\n",
      "0.3               |0.3               |dropout\n",
      "128               |96                |dense_units\n",
      "0.2               |0.3               |dropout_2\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0599 - val_loss: 0.3596\n",
      "Epoch 2/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0118 - val_loss: 0.3121\n",
      "Epoch 3/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0054 - val_loss: 0.3420\n",
      "Epoch 4/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0039 - val_loss: 0.3059\n",
      "Epoch 5/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0032 - val_loss: 0.2554\n",
      "Epoch 6/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0022 - val_loss: 0.2423\n",
      "Epoch 7/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 0.2234\n",
      "Epoch 8/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 0.1981\n",
      "Epoch 9/20\n",
      "\u001b[1m15/62\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 8.2166e-04"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Attention, Dropout, BatchNormalization\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import kerastuner as kt\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Load Dataset\n",
    "dataset_path = '/home/hoangbaoan1901/Documents/information-system_UET/sem5/prdan/tianen101/datasets/jrfm-2156907-cleaned.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Define the target variable\n",
    "target = 'BTC_Close'\n",
    "\n",
    "# Convert date column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Define new training and testing periods\n",
    "start_day_p2 = pd.Timestamp(2018, 10, 1)\n",
    "test_day_p2 = pd.Timestamp(2021, 10, 1)\n",
    "end_day_p2 = pd.Timestamp(2022, 4, 1)\n",
    "\n",
    "# Filter data for the specified period\n",
    "period2 = df[(df['Date'] <= end_day_p2)].reset_index(drop=True)\n",
    "train_dataset_p2 = period2[period2['Date'] < test_day_p2].reset_index(drop=True)\n",
    "test_dataset_p2 = period2[period2['Date'] >= test_day_p2].reset_index(drop=True)\n",
    "train_dataset_p2 = train_dataset_p2.drop(columns=['Date'])\n",
    "test_dataset_p2 = test_dataset_p2.drop(columns=['Date'])\n",
    "\n",
    "# Create the feature sets\n",
    "X_train = train_dataset_p2[:-1]\n",
    "X_test = test_dataset_p2[:-1]\n",
    "\n",
    "# Create the target sets\n",
    "Y_train = train_dataset_p2[target][1:]\n",
    "Y_test = test_dataset_p2[target][1:]\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(df[target].values.reshape(-1, 1))\n",
    "\n",
    "# Prepare data for GRU\n",
    "sequence_length = 60\n",
    "def create_sequences(data, seq_length):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x, y = create_sequences(data_normalized, sequence_length)\n",
    "\n",
    "# Split into training and testing data based on date\n",
    "test_index = len(train_dataset_p2) - sequence_length\n",
    "x_train = x[:test_index]\n",
    "y_train = y[:test_index]\n",
    "x_test = x[test_index:]\n",
    "y_test = y[test_index:]\n",
    "\n",
    "# Hyperparameter Optimization using Keras Tuner\n",
    "def build_model(hp):\n",
    "    inputs = Input(shape=(sequence_length, 1))\n",
    "    # Hyperparameter tuning for the number of units and dropout rates\n",
    "    gru_output = GRU(hp.Int('units', min_value=32, max_value=256, step=32), return_sequences=False)(inputs)\n",
    "    gru_output = BatchNormalization()(gru_output)\n",
    "    gru_output = Dropout(hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1))(gru_output)\n",
    "    \n",
    "    dense_output = Dense(hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu')(gru_output)\n",
    "    dense_output = Dropout(hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1))(dense_output)\n",
    "    dense_output = Dense(1)(dense_output)\n",
    "\n",
    "    # Compile GRU model\n",
    "    model = Model(inputs, dense_output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory='gru_xgb_tuner',\n",
    "    project_name='gru_xgb'\n",
    ")\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Search for best hyperparameters\n",
    "tuner.search(x_train, y_train, epochs=20, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Get the best hyperparameters\n",
    "gru_best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print('Best GRU Hyperparameters:')\n",
    "print(gru_best_hyperparameters.values)\n",
    "\n",
    "# Get the best model\n",
    "gru_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best GRU model\n",
    "gru_model.fit(x_train, y_train, epochs=50, batch_size=16, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Extract features from GRU model\n",
    "feature_extractor = Model(inputs=gru_model.input, outputs=gru_model.layers[-3].output)\n",
    "train_features = feature_extractor.predict(x_train)\n",
    "test_features = feature_extractor.predict(x_test)\n",
    "\n",
    "# Set up hyperparameter optimization for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.005, 0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train XGBoost with optimized hyperparameters\n",
    "random_search.fit(train_features, y_train)\n",
    "\n",
    "# Print best hyperparameters for XGBoost\n",
    "print('Best XGBoost Hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "xgb_best = random_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_best.predict(test_features)\n",
    "\n",
    "# Rescale predictions\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Evaluation\n",
    "rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# Plot the predictions against the actual values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_rescaled, color='blue', label='Actual BTC_Close')\n",
    "plt.plot(y_pred_rescaled, color='red', label='Predicted BTC_Close', linestyle='dashed')\n",
    "plt.title('BTC_Close Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('BTC_Close Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
