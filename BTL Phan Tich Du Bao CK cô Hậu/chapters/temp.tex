% \begin{savequote}[75mm] 
% Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
% \qauthor{Quoteauthor Lastname} 
% \end{savequote}

% \chapter{Consectetuer adipiscing elit}

% \newthought{Lorem ipsum dolor sit amet}, consectetuer adipiscing elit. Morbi commodo, ipsum sed pharetra gravida, orci magna rhoncus neque, id pulvinar odio lorem non turpis. Nullam sit amet enim. Suspendisse id velit vitae ligula volutpat condimentum. Aliquam erat volutpat. Sed quis velit. Nulla facilisi. Nulla libero. Vivamus pharetra posuere sapien. Nam consectetuer. Sed aliquam, nunc eget euismod ullamcorper, lectus nunc ullamcorper orci, fermentum bibendum enim nibh eget ipsum. Donec porttitor ligula eu dolor. Maecenas vitae nulla consequat libero cursus venenatis. Nam magna enim, accumsan eu, blandit sed, blandit a, eros.

% \blindtext

% \section{This is section one}
% \blindtext
% \blindmathpaper

% \section{This is section two}
% \blindtext
% \blindmathpaper
\chapter{Phương pháp}
\section{Long short term memory (LSTM)}
\subsection{Kiến thức nền tảng}
\section{Background Knowledge về RNN}

Recurrent Neural Networks (RNN) là một loại mạng nơ-ron nhân tạo được thiết kế đặc biệt để xử lý dữ liệu tuần tự, như chuỗi thời gian, văn bản, hoặc tín hiệu âm thanh. Điểm đặc biệt của RNN là khả năng ghi nhớ thông tin từ quá khứ thông qua trạng thái ẩn (hidden state), điều này cho phép mô hình học các phụ thuộc dài hạn trong dữ liệu tuần tự. Hãy tưởng tượng bạn đang đọc một câu, mỗi từ bạn đọc đều có mối liên hệ với các từ trước đó. RNN có khả năng duy trì thông tin này thông qua các bước thời gian, giúp nó trở nên hữu ích cho những bài toán liên quan đến dữ liệu tuần tự.

\subsection{Kiến Trúc Cơ Bản của RNN}

Một RNN cơ bản có thể được biểu diễn bằng công thức sau đây:

\begin{equation}
    h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
\end{equation}

\begin{equation}
    y_t = g(W_{hy} h_t + b_y)
\end{equation}

Trong đó:
- $h_t$: Trạng thái ẩn tại thời điểm $t$, biểu diễn thông tin từ đầu vào hiện tại $x_t$ và trạng thái ẩn trước đó $h_{t-1}$.
- $x_t$: Đầu vào tại thời điểm $t$.
- $W_{hh}$, $W_{xh}$, $W_{hy}$: Các ma trận trọng số kết nối giữa các đơn vị trong mạng.
- $b_h$, $b_y$: Các bias.
- $f$ và $g$: Các hàm kích hoạt, thường sử dụng hàm $tanh$ hoặc $ReLU$ cho $f$ và $softmax$ cho $g$.

Ở mỗi bước thời gian $t$, trạng thái ẩn $h_t$ được cập nhật dựa trên đầu vào hiện tại $x_t$ và trạng thái ẩn từ bước trước đó $h_{t-1}$. Điều này cho phép RNN có khả năng “ghi nhớ” thông tin từ quá khứ và sử dụng nó để xử lý đầu vào hiện tại. Đầu ra $y_t$ thường được tính toán dựa trên trạng thái ẩn $h_t$, tùy thuộc vào bài toán dự đoán.

\subsection{Lan Truyền Ngược Qua Thời Gian (BPTT)}

RNN được huấn luyện bằng cách sử dụng kỹ thuật lan truyền ngược qua thời gian (Backpropagation Through Time - BPTT). Trong quá trình này, các gradient được tính toán qua tất cả các bước thời gian. Tuy nhiên, quá trình này gặp phải hai vấn đề lớn là gradient biến mất (vanishing gradient) và gradient bùng nổ (exploding gradient).

- **Gradient Biến Mất**: Khi các gradient trở nên rất nhỏ, quá trình huấn luyện có thể bị ngưng trệ vì các tham số không được cập nhật đáng kể. Điều này xảy ra do khi tính toán gradient qua nhiều bước thời gian, các giá trị gradient nhỏ dần qua từng bước, dẫn đến việc không thể học được các phụ thuộc dài hạn.

- **Gradient Bùng Nổ**: Khi các gradient trở nên quá lớn, giá trị cập nhật của các tham số có thể dẫn đến sự không ổn định trong quá trình huấn luyện, gây ra hiện tượng mất cân bằng.

Để giảm thiểu các vấn đề này, người ta thường sử dụng các kỹ thuật như **gradient clipping**, tức là giới hạn giá trị gradient trong một khoảng cố định để tránh tình trạng bùng nổ.

\subsection{Các Mô Hình Nâng Cao: LSTM và GRU}

Để khắc phục các hạn chế của RNN cơ bản, các kiến trúc nâng cao hơn như Long Short-Term Memory (LSTM) và Gated Recurrent Unit (GRU) đã được phát triển. 

\subsubsection{LSTM}

LSTM là một loại RNN được thiết kế để giải quyết vấn đề gradient biến mất, nhờ vào các cổng kiểm soát (gates) cho phép mô hình lưu trữ hoặc loại bỏ thông tin một cách có chọn lọc. Cấu trúc của LSTM bao gồm ba cổng chính:
- **Cổng quên (Forget Gate)**: Quyết định thông tin nào từ trạng thái trước đó cần được quên.
- **Cổng nhập (Input Gate)**: Quyết định thông tin nào sẽ được lưu trữ vào trạng thái hiện tại.
- **Cổng đầu ra (Output Gate)**: Quyết định thông tin nào từ trạng thái hiện tại sẽ được sử dụng để tính đầu ra.

Các công thức của LSTM có thể được mô tả như sau:

\begin{equation}
    f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)
\end{equation}

\begin{equation}
    i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)
\end{equation}

\begin{equation}
    \tilde{C}_t = \tanh(W_C [h_{t-1}, x_t] + b_C)
\end{equation}

\begin{equation}
    C_t = f_t * C_{t-1} + i_t * \tilde{C}_t
\end{equation}

\begin{equation}
    o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)
\end{equation}

\begin{equation}
    h_t = o_t * \tanh(C_t)
\end{equation}

Trong đó, $f_t$, $i_t$, và $o_t$ lần lượt là cổng quên, cổng nhập, và cổng đầu ra. Trạng thái ẩn $h_t$ và trạng thái tế bào $C_t$ được cập nhật qua các bước thời gian, giúp LSTM duy trì thông tin dài hạn tốt hơn so với RNN thông thường.

\subsubsection{GRU}

GRU là một biến thể khác của RNN, có cấu trúc đơn giản hơn LSTM nhưng vẫn giữ được khả năng học các phụ thuộc dài hạn. GRU kết hợp các cổng vào và cổng quên thành một cổng duy nhất, giúp giảm số lượng tham số cần phải huấn luyện. Công thức của GRU như sau:

\begin{equation}
    r_t = \sigma(W_r [h_{t-1}, x_t] + b_r)
\end{equation}

\begin{equation}
    z_t = \sigma(W_z [h_{t-1}, x_t] + b_z)
\end{equation}

\begin{equation}
    \tilde{h}_t = \tanh(W_h [r_t * h_{t-1}, x_t] + b_h)
\end{equation}

\begin{equation}
    h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t
\end{equation}

Trong đó, $r_t$ và $z_t$ lần lượt là cổng đặt lại (reset gate) và cổng cập nhật (update gate). GRU sử dụng ít cổng hơn so với LSTM, do đó quá trình huấn luyện thường nhanh hơn và yêu cầu ít tài nguyên hơn.

\subsection{Ứng Dụng của RNN}

RNN và các biến thể của nó có nhiều ứng dụng quan trọng trong lĩnh vực trí tuệ nhân tạo, đặc biệt là trong xử lý ngôn ngữ tự nhiên (NLP) và phân tích chuỗi thời gian. Một số ứng dụng tiêu biểu của RNN bao gồm:
- **Mô hình hóa ngôn ngữ**: Dự đoán từ tiếp theo trong một câu.
- **Nhận diện chữ viết tay**: Xử lý chuỗi dữ liệu từ các nét viết tay.
- **Dịch máy**: Dịch văn bản từ ngôn ngữ này sang ngôn ngữ khác.
- **Phân tích chuỗi thời gian**: Dự đoán giá cổ phiếu, phân tích dữ liệu cảm biến, v.v.

Mặc dù RNN có một số hạn chế, các mô hình nâng cao như LSTM và GRU đã giúp cải thiện đáng kể hiệu quả và khả năng ứng dụng của RNN trong các bài toán phức tạp liên quan đến dữ liệu tuần tự.
